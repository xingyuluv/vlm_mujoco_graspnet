import os
import sys
import cv2
import numpy as np
import open3d as o3d
import torch
import time
import gc

# ================= è·¯å¾„é…ç½® =================
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

MANIPULATOR_PATH = os.path.join(PROJECT_ROOT, 'manipulator_grasp')
if MANIPULATOR_PATH not in sys.path:
    sys.path.append(MANIPULATOR_PATH)

GRASPNET_ROOT = os.path.join(PROJECT_ROOT, 'graspnet-baseline')
sys.path.append(os.path.join(GRASPNET_ROOT, 'models'))
sys.path.append(os.path.join(GRASPNET_ROOT, 'dataset'))
sys.path.append(os.path.join(GRASPNET_ROOT, 'utils'))

try:
    from manipulator_grasp.env.ur5_grasp_old_env import UR5GraspEnv
    # ã€ä¿®æ”¹ã€‘ä» vlm_process å¯¼å…¥ YOLO å’Œ SAM ç›¸å…³å‡½æ•°
    from vlm_process import get_yolo_model, get_sam_predictor, process_sam_results
    from grasp_process_old import get_net, get_and_process_data
    from graspnetAPI import GraspGroup
    from graspnet import pred_decode
    from collision_detector import ModelFreeCollisionDetector
    print("âœ… æˆåŠŸåŠ è½½ç¯å¢ƒä¸ç®—æ³•æ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# å…¨å±€å˜é‡
is_processing = False

def get_camera_intrinsic(width, height):
    fovy = 45 
    f = 0.5 * height / np.tan(fovy * np.pi / 360)
    return o3d.camera.PinholeCameraIntrinsic(width, height, f, f, width / 2, height / 2)

def create_point_cloud(rgb, depth, intrinsics):
    """
    ç”Ÿæˆç”¨äºæ˜¾ç¤ºçš„ç‚¹äº‘
    """
    depth_o3d = o3d.geometry.Image(depth)
    color_o3d = o3d.geometry.Image(cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB))
    
    # åˆ›å»º RGBD å›¾
    rgbd = o3d.geometry.RGBDImage.create_from_color_and_depth(
        color_o3d, depth_o3d, 
        depth_scale=1.0,  # MuJoCo è¾“å‡ºå•ä½æ˜¯ç±³ï¼Œè¿™é‡Œä¿æŒ 1.0
        depth_trunc=2.0, 
        convert_rgb_to_intensity=False
    )
    
    # ç”Ÿæˆç‚¹äº‘
    pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd, intrinsics)
    return pcd

def inference_full(net, rgb, depth, mask):
    # 1. æ•°æ®é¢„å¤„ç†
    end_points, cloud_o3d = get_and_process_data(rgb, depth, mask)

    # 2. å‰å‘æ¨ç† (æ— æ¢¯åº¦)
    with torch.no_grad():
        end_points = net(end_points)
        grasp_preds = pred_decode(end_points)

    # 3. æ„é€ ç»“æœ
    preds_np = grasp_preds[0].detach().cpu().numpy()
    gg = GraspGroup(preds_np)

    # 4. æ¸…ç†ä¸­é—´æ˜¾å­˜
    del end_points, grasp_preds
    
    # 5. ç¢°æ’æ£€æµ‹
    collision_thresh = 0.01
    voxel_size = 0.01
    mfcdetector = ModelFreeCollisionDetector(np.asarray(cloud_o3d.points), voxel_size=voxel_size)
    collision_mask = mfcdetector.detect(gg, approach_dist=0.05, collision_thresh=collision_thresh)
    gg = gg[~collision_mask]

    # 6. NMS å’Œæ’åº
    gg = gg.nms()
    gg = gg.sort_by_score()

    # 7. å‚ç›´è§’åº¦è¿‡æ»¤
    all_grasps = list(gg)
    vertical = np.array([0, 0, 1]) 
    angle_threshold = np.deg2rad(30) 
    
    filtered_list = []
    for grasp in all_grasps:
        approach_dir = grasp.rotation_matrix[:, 0]
        cos_angle = np.dot(approach_dir, vertical)
        cos_angle = np.clip(cos_angle, -1.0, 1.0)
        angle = np.arccos(cos_angle)
        if angle < angle_threshold:
            filtered_list.append(grasp)
            
    if len(filtered_list) == 0:
        final_gg = gg
    else:
        print(f"[Info] å‚ç›´è¿‡æ»¤: {len(all_grasps)} -> {len(filtered_list)}")
        grasp_array = np.array([g.grasp_array for g in filtered_list])
        final_gg = GraspGroup(grasp_array)

    return final_gg

def main():
    global is_processing
    
    # æ¸…ç†ç¯å¢ƒ
    gc.collect()
    torch.cuda.empty_cache()
    
    print("ğŸš€ æ­£åœ¨å¯åŠ¨ MuJoCo ç¯å¢ƒ...")
    env = UR5GraspEnv()
    env.reset()
    
    # ================= å…³é”®ä¿®å¤å¼€å§‹ =================
    # å®šä¹‰ä¸€ä¸ªâ€œä¿æŒåŠ¨ä½œâ€ï¼Œè®©æœºæ¢°è‡‚åœåœ¨åˆå§‹ä½ç½®ï¼Œä¸è¦å€’ä¸‹æ¥æŒ¡é•œå¤´
    home_action = np.zeros(7)
    # è·å– UR5GraspEnv reset åçš„é»˜è®¤å…³èŠ‚è§’ (é€šå¸¸æ˜¯ä¸Šæ–¹æ¨ªç½®çŠ¶æ€)
    home_action[:6] = env.robot_q 
    # ================= å…³é”®ä¿®å¤ç»“æŸ =================
    
    print("â³ ç‰©ç†é¢„çƒ­ (200æ­¥)...")
    for _ in range(200): 
        env.step(home_action) # <--- ä¼ å…¥åŠ¨ä½œ
    
    print("ğŸ”„ åŠ è½½æ¨¡å‹ (YOLO-World + SAM + GraspNet)...")
    yolo_model = get_yolo_model()
    sam_predictor = get_sam_predictor()
    grasp_net = get_net() 
    print("âœ… æ¨¡å‹å°±ç»ª")

    window_name = "MuJoCo View"
    cv2.namedWindow(window_name)
    
    print("\n" + "="*50)
    print("ğŸ® æ“ä½œæŒ‡å—:")
    print("1. æŒ‰é”®ç›˜ 'f' é”® -> è¾“å…¥ç‰©ä½“åç§°è¿›è¡Œæ£€æµ‹")
    print("2. æŒ‰é”®ç›˜ 'q' é”® -> é€€å‡º")
    print("3. åœ¨ Open3D çª—å£ä¸­ï¼Œç”¨é¼ æ ‡æ—‹è½¬è§†è§’æŸ¥çœ‹æŠ“å–ä½å§¿")
    print("="*50 + "\n")

    while True:
        # ================= å…³é”®ä¿®å¤ =================
        env.step(home_action) # <--- æŒç»­ä¼ å…¥åŠ¨ä½œï¼Œå›ºå®šæœºæ¢°è‡‚
        # ===========================================
        
        obs = env.render()
        rgb = cv2.cvtColor(obs['img'], cv2.COLOR_RGB2BGR)
        depth = obs['depth']
        
        vis_img = rgb.copy()
        cv2.putText(vis_img, "Press 'f' to Find Object", (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        cv2.imshow(window_name, vis_img)
        key = cv2.waitKey(10)
        
        if key == ord('q'): 
            break
        
        if key == ord('f') and not is_processing:
            is_processing = True
            print("\nğŸ“ è¯·è¾“å…¥ç›®æ ‡ç‰©ä½“åç§° (ä¾‹å¦‚: apple, box)")
            try:
                text_prompt = input("ğŸ‘‰ è¯·è¾“å…¥: ").strip()
            except EOFError:
                text_prompt = ""
            
            if not text_prompt:
                print("âš ï¸ è¾“å…¥ä¸ºç©ºï¼Œå–æ¶ˆæ“ä½œã€‚")
                is_processing = False
                continue

            print(f"ğŸ” YOLO-World æ­£åœ¨æœç´¢: '{text_prompt}' ...")
            
            try:
                yolo_model.set_classes([text_prompt])
                with torch.no_grad():
                    results = yolo_model.predict(rgb, conf=0.05, iou=0.5, verbose=False)
                
                bbox = None
                if len(results) > 0 and len(results[0].boxes) > 0:
                    best_box = results[0].boxes[0]
                    coords = best_box.xyxy[0].cpu().numpy().astype(int)
                    conf = float(best_box.conf)
                    bbox = coords.tolist()
                    print(f"âœ… æ‰¾åˆ°ç›®æ ‡! ç½®ä¿¡åº¦: {conf:.2f}, BBox: {bbox}")
                    
                    cv2.rectangle(vis_img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)
                    cv2.imshow(window_name, vis_img)
                    cv2.waitKey(10)
                else:
                    print(f"âŒ æœªæ‰¾åˆ°ç›®æ ‡: '{text_prompt}'")
                    is_processing = False
                    continue

                print("ğŸ”„ å¯åŠ¨ SAM åˆ†å‰²...")
                image_rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)
                with torch.no_grad():
                    sam_predictor.set_image(image_rgb)
                    sam_results = sam_predictor(bboxes=[bbox], points=None, labels=None)
                
                _, mask = process_sam_results(sam_results)
                del sam_results
                
                if mask is not None:
                    mask_vis = cv2.addWeighted(rgb, 0.7, cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR), 0.3, 0)
                    cv2.imshow("Mask Result", mask_vis)
                    cv2.waitKey(100)
                    
                    print("ğŸ¤– è®¡ç®—æŠ“å–ç‚¹...")
                    # è¿™é‡Œçš„ inference_full ä¼šè°ƒç”¨ GraspNet
                    # ç”±äºæœºæ¢°è‡‚ç°åœ¨ä¸å†é®æŒ¡ï¼Œç”Ÿæˆçš„æŠ“å–ç‚¹åº”è¯¥ä¼šæš´å¢åˆ° 300+
                    gg = inference_full(grasp_net, rgb, depth, mask)
                    
                    if len(gg) > 0:
                        gg = gg.sort_by_score()
                        gg_vis = gg[:50] 
                        print(f"âœ… æ˜¾ç¤º Top-{len(gg_vis)} ä¸ªæŠ“å–ç‚¹")
                        
                        h, w = rgb.shape[:2]
                        intrinsics = get_camera_intrinsic(w, h)
                        cloud = create_point_cloud(rgb, depth, intrinsics)
                        grippers = gg_vis.to_open3d_geometry_list()
                        for gripper in grippers:
                            gripper.paint_uniform_color([1, 0, 0])

                        o3d.visualization.draw_geometries([cloud, *grippers], 
                                                          window_name=f"Detection: {text_prompt} | Top {len(gg_vis)} Grasps")
                    else:
                        print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆæŠ“å–")
                    del gg
                else:
                    print("âŒ SAM åˆ†å‰²å¤±è´¥")

            except Exception as e:
                print(f"âŒ é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
            
            finally:
                is_processing = False
                gc.collect()
                torch.cuda.empty_cache()
                print("â™»ï¸  å°±ç»ªã€‚\n")

    env.close()
    cv2.destroyAllWindows()

if __name__ == '__main__':
    main()