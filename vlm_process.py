import cv2
import numpy as np
import torch
from ultralytics.models.sam import Predictor as SAMPredictor

import whisper
import json
import re
import base64
import textwrap
import queue
import time
import io
import os
import asyncio  # æ–°å¢ï¼šç”¨äºè¿è¡Œå¼‚æ­¥çš„ edge-tts

import soundfile as sf  
import sounddevice as sd
from scipy.io.wavfile import write
from pydub import AudioSegment

import edge_tts  # æ–°å¢ï¼šå¯¼å…¥ edge-tts åº“

from openai import OpenAI  # å¯¼å…¥OpenAIå®¢æˆ·ç«¯

import logging
# ç¦ç”¨ Ultralytics çš„æ—¥å¿—è¾“å‡º
logging.getLogger("ultralytics").setLevel(logging.WARNING)


# ----------------------- åŸºç¡€å·¥å…·å‡½æ•° -----------------------

def encode_np_array(image_np):
    """å°† numpy å›¾åƒæ•°ç»„ï¼ˆBGRï¼‰ç¼–ç ä¸º base64 å­—ç¬¦ä¸²"""
    success, buffer = cv2.imencode('.jpg', image_np)
    if not success:
        raise ValueError("æ— æ³•å°†å›¾åƒæ•°ç»„ç¼–ç ä¸º JPEG")
    img_base64 = base64.b64encode(buffer).decode('utf-8')
    return img_base64



# ----------------------- å¤šæ¨¡æ€æ¨¡å‹è°ƒç”¨ï¼ˆQwenï¼‰ -----------------------

def generate_robot_actions(user_command, image_input=None):
    """
    ä½¿ç”¨ base64 çš„æ–¹å¼å°† numpy å›¾åƒå’Œç”¨æˆ·æ–‡æœ¬æŒ‡ä»¤ä¼ ç»™ Qwen å¤šæ¨¡æ€æ¨¡å‹ï¼Œ
    """
    # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
    # æ›¿æ¢ä¸ºè‡ªå·±çš„æ¨¡å‹è°ƒç”¨
    client = OpenAI(api_key='sk-92e5d2bbb4324174b0c5158fface3c78', base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")

    system_prompt = textwrap.dedent("""\
    ä½ æ˜¯ä¸€ä¸ªç²¾å¯†æœºæ¢°è‡‚è§†è§‰æ§åˆ¶ç³»ç»Ÿï¼Œå…·å¤‡å…ˆè¿›çš„å¤šæ¨¡æ€æ„ŸçŸ¥èƒ½åŠ›ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œä»»åŠ¡ï¼š

    ã€å›¾åƒåˆ†æé˜¶æ®µã€‘
    1. åˆ†æè¾“å…¥å›¾åƒï¼Œè¯†åˆ«å›¾åƒä¸­æ‰€æœ‰å¯è§ç‰©ä½“ï¼Œå¹¶è®°å½•æ¯ä¸ªç‰©ä½“çš„è¾¹ç•Œæ¡†ï¼ˆå·¦ä¸Šè§’ç‚¹å’Œå³ä¸‹è§’ç‚¹ï¼‰åŠå…¶ç±»åˆ«åç§°ã€‚

    ã€æŒ‡ä»¤è§£æé˜¶æ®µã€‘
    2. æ ¹æ®ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œä»è¯†åˆ«çš„ç‰©ä½“ä¸­ç­›é€‰å‡ºæœ€åŒ¹é…çš„ç›®æ ‡ç‰©ä½“ã€‚

    ã€å“åº”ç”Ÿæˆé˜¶æ®µã€‘
    3. è¾“å‡ºæ ¼å¼å¿…é¡»ä¸¥æ ¼å¦‚ä¸‹ï¼š
    - è‡ªç„¶è¯­è¨€å“åº”ï¼ˆä»…åŒ…å«è¯´æ˜ä¸ºä½•é€‰æ‹©è¯¥ç‰©ä½“çš„æ–‡å­—,å¯ä»¥ä¿çš®å¯çˆ±åœ°å›åº”ç”¨æˆ·çš„éœ€æ±‚ï¼Œä½†æ˜¯è¯·æ³¨æ„ï¼Œå›ç­”ä¸­åº”è¯¥åªåŒ…å«è¢«é€‰ä¸­çš„ç‰©ä½“ï¼‰ï¼Œ
    - ç´§è·Ÿå…¶åï¼Œä»ä¸‹ä¸€è¡Œå¼€å§‹è¿”å› **æ ‡å‡† JSON å¯¹è±¡**,ä½†æ˜¯ä¸è¦è¿”å›jsonæœ¬ä½“,æ ¼å¼å¦‚ä¸‹ï¼š

    {
      "name": "ç‰©ä½“åç§°",
      "bbox": [å·¦ä¸Šè§’x, å·¦ä¸Šè§’y, å³ä¸‹è§’x, å³ä¸‹è§’y]
    }

    ã€æ³¨æ„äº‹é¡¹ã€‘
    - JSON å¿…é¡»ä»ä¸‹ä¸€è¡Œå¼€å§‹ï¼›
    - è‡ªç„¶è¯­è¨€å“åº”ä¸ JSON ä¹‹é—´æ— å…¶ä»–é¢å¤–æ–‡æœ¬;
    - JSON å¯¹è±¡ä¸èƒ½æœ‰ä»»ä½•æ³¨é‡Šã€é¢å¤–æ–‡æœ¬æˆ–è§£é‡Š,åŒ…æ‹¬ä¸èƒ½æœ‰è¾…åŠ©æ ‡è¯†ä¸ºjsonæ–‡æœ¬çš„å†…å®¹,ä¸è¦æœ‰json;
    - åæ ‡ bbox å¿…é¡»ä¸ºæ•´æ•°ï¼›
    - åœ¨æŠ“å–å¸¦æ¡æŠŠçš„å·¥å…·æ—¶ï¼Œä¼˜å…ˆæŠ“å–æ¡æŠŠï¼›                              
    - åªå…è®¸ä½¿ç”¨ "bbox" ä½œä¸ºåæ ‡æ ¼å¼ã€‚
    """)

    messages = [{"role": "system", "content": system_prompt}]
    user_content = []

    if image_input is not None:
        base64_img = encode_np_array(image_input)
        user_content.append({
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{base64_img}"
            }
        })

    user_content.append({"type": "text", "text": user_command})
    messages.append({"role": "user", "content": user_content})

    try:
        # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯è°ƒç”¨API
        completion = client.chat.completions.create(
            model="qwen-vl-plus", 
            messages=messages,
            temperature=0.1, 
        )
        
        content = completion.choices[0].message.content
        print("åŸå§‹å“åº”ï¼š", content)

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾ JSON éƒ¨åˆ†
        match = re.search(r'(\{.*\})', content, re.DOTALL)
        if match:
            json_str = match.group(1)
            try:
                coord = json.loads(json_str)
            except Exception as e:
                print(f"[è­¦å‘Š] JSON è§£æå¤±è´¥ï¼š{e}")
                coord = {}
            natural_response = content[:match.start()].strip()
        else:
            natural_response = content.strip()
            coord = {}

        return {
            "response": natural_response,
            "coordinates": coord
        }

    except Exception as e:
        print(f"è¯·æ±‚å¤±è´¥ï¼š{e}")
        return {"response": "å¤„ç†å¤±è´¥", "coordinates": {}}

# ----------------------- SAM åˆ†å‰²ç›¸å…³ -----------------------
def choose_model():
    """Initialize SAM predictor with proper parameters"""
    model_weight = 'sam_b.pt'
    overrides = dict(
        task='segment',
        mode='predict',
        # imgsz=1024,
        model=model_weight,
        conf=0.25,
        save=False
    )
    return SAMPredictor(overrides=overrides)

def process_sam_results(results):
    """Process SAM results to get mask and center point"""
    if not results or not results[0].masks:
        return None, None

    # Get first mask (assuming single object segmentation)
    mask = results[0].masks.data[0].cpu().numpy()
    mask = (mask > 0).astype(np.uint8) * 255

    # Find contour and center
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        return None, None

    M = cv2.moments(contours[0])
    if M["m00"] == 0:
        return None, mask

    cx = int(M["m10"] / M["m00"])
    cy = int(M["m01"] / M["m00"])
    return (cx, cy), mask


# ----------------------- è¯­éŸ³è¯†åˆ«ä¸ TTS (Edge-TTS ä¿®æ”¹ç‰ˆ) -----------------------

# åˆå§‹åŒ–å…¨å±€æ¨¡å‹å˜é‡
_global_models = {}

def load_models():
    """åœ¨éœ€è¦æ—¶åŠ è½½æ¨¡å‹ï¼Œé¿å…å¯åŠ¨æ—¶å…¨éƒ¨åŠ è½½å ç”¨èµ„æº"""
    if not _global_models:
        print("ğŸ”„ æ­£åœ¨åŠ è½½ç¦»çº¿è¯­éŸ³æ¨¡å‹...")
        # åŠ è½½Whisperå°å‹æ¨¡å‹ (é€‚åˆä½ çš„6GBæ˜¾å­˜)
        # å®é™…ä½¿ç”¨æ—¶è¯·å–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
        _global_models['asr'] = whisper.load_model("small")
        print("âœ… Whisperæ¨¡å‹åŠ è½½å®Œæ¯• (æ¨¡æ‹Ÿ)")
        
        # æ³¨æ„ï¼šEdge-TTS æ˜¯åœ¨çº¿/å¼‚æ­¥åº“ï¼Œä¸éœ€è¦åƒ pyttsx3 é‚£æ ·åœ¨æ­¤å¤„åˆå§‹åŒ–å¯¹è±¡
        
    return _global_models


# éŸ³é¢‘å‚æ•°é…ç½®
samplerate = 48000
channels = 1
dtype = 'int16'
frame_duration = 0.2
frame_samples = int(frame_duration * samplerate)
silence_threshold = 250
silence_max_duration = 2.0
q = queue.Queue()


def rms(audio_frame):
    samples = np.frombuffer(audio_frame, dtype=np.int16)
    if samples.size == 0:
        return 0
    mean_square = np.mean(samples.astype(np.float32) ** 2)
    if np.isnan(mean_square) or mean_square < 1e-5:
        return 0
    return np.sqrt(mean_square)

def callback(indata, frames, time_info, status):
    if status:
        print("âš ï¸ çŠ¶æ€è­¦å‘Šï¼š", status)
    q.put(bytes(indata))

def recognize_speech():
    """
    ã€å¾®è·é˜ˆå€¼ç‰ˆã€‘
    é’ˆå¯¹é«˜åº•å™ª(7500)ã€ä½äººå£°(8500)çš„æé™ç¯å¢ƒè®¾è®¡ã€‚
    å°†é˜ˆå€¼ç²¾å‡†å¡åœ¨ä¸¤è€…ä¹‹é—´ (çº¦ 8000)ã€‚
    """
    
    # === æ ¸å¿ƒé…ç½® ===
    DEVICE_ID = 13           
    
    # ã€å…³é”®ç­–ç•¥ã€‘
    # ä½ çš„åº•å™ªæ˜¯ 7500ï¼Œäººå£°æ˜¯ 8500
    # æˆ‘ä»¬éœ€è¦è®©é˜ˆå€¼åŠ¨æ€åœ°è´´åœ¨åº•å™ªä¸Šé¢ä¸€ç‚¹ç‚¹
    # æ¯”å¦‚: åº•å™ª + 500 = 8000
    NOISE_MARGIN = 500       
    
    # ç¡¬ä¿åº•ï¼šæ— è®ºå¦‚ä½•ï¼Œé˜ˆå€¼ä¸èƒ½ä½äº 7600 (é˜²æ­¢è¯¯è§¦)
    # ä¹Ÿä¸èƒ½é«˜äº 8400 (é˜²æ­¢ä½ è¯´è¯å¬ä¸è§)
    MIN_SAFE_THRESHOLD = 7600
    MAX_SAFE_THRESHOLD = 8400
    
    BUFFER_DURATION = 1.0    
    CALIBRATION_TIME = 2.0   
    MAX_RECORD_TIME = 15.0   
    SILENCE_TIMEOUT = 1.2    # ç¨å¾®ç¼©çŸ­ï¼Œååº”å¿«ä¸€ç‚¹
    
    local_frame_samples = int(BUFFER_DURATION * samplerate)
    
    with q.mutex:
        q.queue.clear()

    print("\n" + "="*40)
    print("   ğŸ”‡ æ­£åœ¨æµ‹é‡ç¯å¢ƒåº•å™ª (è¯·ä¿æŒç»å¯¹å®‰é™)...")
    print("="*40)
    
    noise_values = []
    
    try:
        # --- é˜¶æ®µ 1: ç²¾å¯†æ ¡å‡† ---
        with sd.RawInputStream(samplerate=samplerate, blocksize=local_frame_samples,
                               device=DEVICE_ID, latency='high',
                               dtype=dtype, channels=channels, callback=callback):
            
            # ç­‰å¾…ä¸€å°ä¼šå„¿è®©æ•°æ®ç¨³å®š
            time.sleep(0.5)
            
            for _ in range(int(CALIBRATION_TIME / BUFFER_DURATION)):
                if not q.empty():
                    frame = q.get()
                    val = rms(frame)
                    noise_values.append(val)
                    print(f"   ... é‡‡æ ·åº•å™ª: {int(val)}")
                else:
                    time.sleep(BUFFER_DURATION)
            
        avg_noise = np.mean(noise_values) if noise_values else 7500
        
        # ã€æ ¸å¿ƒç®—æ³•ã€‘
        # è®¡ç®—ç›®æ ‡é˜ˆå€¼ï¼šåº•å™ª + 500
        calculated_threshold = avg_noise + NOISE_MARGIN
        
        # ã€åŒé‡ä¿é™©ã€‘
        # 1. å³ä½¿åº•å™ªå¾ˆå°ï¼Œé˜ˆå€¼ä¹Ÿä¸èƒ½ä½äº MIN_SAFE_THRESHOLD
        # 2. å³ä½¿åº•å™ªå¾ˆå¤§ï¼Œé˜ˆå€¼ä¹Ÿä¸èƒ½è¶…è¿‡ MAX_SAFE_THRESHOLD (å¦åˆ™ä½ è¯´è¯å°±è§¦å‘ä¸äº†äº†)
        final_threshold = max(calculated_threshold, MIN_SAFE_THRESHOLD)
        final_threshold = min(final_threshold, MAX_SAFE_THRESHOLD)
        
        print(f"   âœ… åº•å™ª: {int(avg_noise)} | ğŸ¯ é”å®šé˜ˆå€¼: {int(final_threshold)}")
        print(f"   ğŸ¤ è¯·è¯´è¯ (éŸ³é‡éœ€è¶…è¿‡ {int(final_threshold)})...")

        # --- é˜¶æ®µ 2: ç›‘å¬ ---
        audio_buffer = []
        is_speaking = False
        last_voice_time = time.time()
        start_record_time = None
        
        with sd.RawInputStream(samplerate=samplerate, blocksize=local_frame_samples,
                               device=DEVICE_ID, latency='high',
                               dtype=dtype, channels=channels, callback=callback):
            while True:
                frame = q.get() 
                volume = rms(frame)
                current_time = time.time()

                status_icon = "ğŸ”´ REC" if is_speaking else "ğŸ‘‚ WAIT"
                
                # è¿›åº¦æ¡ç¼©æ”¾ (é’ˆå¯¹ 7500-9000 çš„åŒºé—´ä¼˜åŒ–æ˜¾ç¤º)
                # å‡å» 7000 æ˜¯ä¸ºäº†è®©å¾®å°çš„å˜åŒ–åœ¨è¿›åº¦æ¡ä¸Šæ›´æ˜æ˜¾
                display_vol = max(0, volume - 7000)
                bar_len = int((display_vol / 2000) * 20) 
                if bar_len > 20: bar_len = 20
                bar_visual = "â–ˆ" * bar_len
                
                # æ‰“å°è¯¦ç»†å¯¹æ¯”
                info_str = f"{int(volume)} > {int(final_threshold)}?"
                print(f"\r   {status_icon} |{bar_visual:<20}| {info_str}", end="")

                # --- è§¦å‘é€»è¾‘ ---
                if volume > final_threshold:
                    if not is_speaking:
                        is_speaking = True
                        start_record_time = current_time
                        audio_buffer = [] 
                    
                    audio_np = np.frombuffer(frame, dtype=np.int16)
                    audio_buffer.append(audio_np)
                    last_voice_time = current_time
                
                else:
                    if is_speaking:
                        audio_np = np.frombuffer(frame, dtype=np.int16)
                        audio_buffer.append(audio_np)

                        if current_time - last_voice_time > SILENCE_TIMEOUT:
                            print(f"\n\n   âœ… æŒ‡ä»¤æ¥æ”¶å®Œæ¯•ã€‚")
                            return np.concatenate(audio_buffer, axis=0)
                        
                        if current_time - start_record_time > MAX_RECORD_TIME:
                            print(f"\n\n   âš ï¸ è¾¾åˆ°æœ€å¤§æ—¶é•¿ï¼Œè‡ªåŠ¨ç»“æŸã€‚")
                            return np.concatenate(audio_buffer, axis=0)

                    elif (current_time - last_voice_time > 30.0): 
                        print("\n\n   ğŸ›‘ è¶…æ—¶æœªæ£€æµ‹åˆ°è¯­éŸ³ã€‚")
                        return np.array([], dtype=np.int16)
                        
    except Exception as e:
        print(f"\nâŒ éŸ³é¢‘è®¾å¤‡é”™è¯¯: {e}")
        return np.array([], dtype=np.int16)
    
def speech_to_text_offline(audio_data):
    """ä½¿ç”¨ç¦»çº¿Whisperæ¨¡å‹å°†å½•éŸ³æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬"""
    print("ğŸ“¡ æ­£åœ¨è¿›è¡Œç¦»çº¿è¯­éŸ³è¯†åˆ«...")
    models = load_models()
    asr_model = models.get('asr')
    
    if not asr_model:
        print("âŒ ASRæ¨¡å‹æœªåŠ è½½")
        return ""

    temp_wav = "temp_audio.wav"
    write(temp_wav, samplerate, audio_data.astype(np.int16))

    try:
        result = asr_model.transcribe(temp_wav, language="zh", fp16=torch.cuda.is_available())
        return result["text"].strip()
        # return "æ¨¡æ‹Ÿè¯†åˆ«ç»“æœï¼šè¯·æŠ“å–é‚£ä¸ªçº¢è‰²çš„æ¯å­" # è°ƒè¯•ç”¨ï¼Œå®é™…è¯·ç”¨ä¸Šé¢ä¸¤è¡Œ
    except Exception as e:
        print(f"âŒ ç¦»çº¿è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
        return ""

# ---- Edge-TTS æ ¸å¿ƒé€»è¾‘ ----

async def _edge_tts_generate(text, output_file, voice="zh-CN-XiaoxiaoNeural"):
    """
    å¼‚æ­¥ç”Ÿæˆè¯­éŸ³æ–‡ä»¶
    Voice å¯é€‰: 
    - zh-CN-XiaoxiaoNeural (å¥³å£°ï¼Œè‡ªç„¶ï¼Œæ¨è)
    - zh-CN-YunxiNeural (ç”·å£°)
    """
    communicate = edge_tts.Communicate(text, voice)
    await communicate.save(output_file)

def play_tts_edge(text):
    """
    ä½¿ç”¨ Edge-TTS ç”Ÿæˆè¯­éŸ³ï¼Œå¼ºåˆ¶é‡é‡‡æ ·ä¸º 48000Hz åæ’­æ”¾
    """
    if not text:
        return
        
    print(f"ğŸ“¢ Edge-TTS æ’­æŠ¥: {text}")
    temp_mp3 = "temp_tts.mp3"
    
    try:
        # 1. å¼‚æ­¥ç”Ÿæˆ MP3 æ–‡ä»¶
        asyncio.run(_edge_tts_generate(text, temp_mp3))
        
        # 2. ä½¿ç”¨ Pydub è¯»å– MP3
        audio = AudioSegment.from_mp3(temp_mp3)
        
        # === ã€æ ¸å¿ƒä¿®å¤ã€‘å¼ºåˆ¶è½¬æ¢ä¸º 48000Hz (æ ‡å‡†é‡‡æ ·ç‡) ===
        target_sr = 48000
        audio = audio.set_frame_rate(target_sr)
        audio = audio.set_channels(1) # å¼ºåˆ¶å•å£°é“ï¼Œå…¼å®¹æ€§æ›´å¥½
        # =================================================
        
        # 3. è½¬æ¢ä¸º Numpy æ•°ç»„
        data = np.array(audio.get_array_of_samples())
        
        # 4. æ’­æ”¾ (ä½¿ç”¨å¼ºåˆ¶è®¾å®šçš„é‡‡æ ·ç‡)
        sd.play(data, target_sr)
        sd.wait() 
        
    except Exception as e:
        print(f"âŒ TTS æ’­æ”¾å¤±è´¥: {e}")
        # å¤‡é€‰æ–¹æ¡ˆï¼šå¦‚æœå£°å¡æå…¶é¡½å›ºï¼Œå¯ä»¥ä½¿ç”¨ Linux ç³»ç»Ÿå‘½ä»¤æ’­æ”¾
        # os.system(f"ffplay -nodisp -autoexit -hide_banner {temp_mp3}")
    finally:
        if os.path.exists(temp_mp3):
            try:
                os.remove(temp_mp3)
            except:
                pass


def voice_command_to_keyword():
    """è·å–è¯­éŸ³å‘½ä»¤å¹¶è½¬æ¢ä¸ºæ–‡æœ¬"""
    audio_data = recognize_speech()
    if len(audio_data) == 0:
        return ""
    text = speech_to_text_offline(audio_data)
    if not text:
        print("âš ï¸ æ²¡æœ‰è¯†åˆ«åˆ°æ–‡æœ¬")
        return ""
    print("ğŸ“ è¯†åˆ«æ–‡æœ¬ï¼š", text)
    return text


# ----------------------- ä¸»æµç¨‹ï¼šå›¾åƒåˆ†å‰² -----------------------
def segment_image(image_input, output_mask='mask1.png'):
    
    # å¦‚æœ image_input æ˜¯è·¯å¾„å­—ç¬¦ä¸²ï¼Œè¯»å–ä¸ºå›¾ç‰‡
    if isinstance(image_input, str):
        image_input = cv2.imread(image_input)
        if image_input is None:
            print("âŒ æ— æ³•è¯»å–å›¾ç‰‡è·¯å¾„")
            return None

    # 1. è·å–æŒ‡ä»¤ (è¿™é‡Œæ¼”ç¤ºç”¨æ–‡å­—è¾“å…¥ï¼Œä¹Ÿå¯åˆ‡æ¢å›è¯­éŸ³)
    print("ğŸ“ è¯·é€šè¿‡æ–‡å­—æè¿°ç›®æ ‡ç‰©ä½“åŠæŠ“å–æŒ‡ä»¤...")
    #command_text = input("è¯·è¾“å…¥: ").strip()
    command_text = voice_command_to_keyword()
    if not command_text:
         command_text = input("è¯­éŸ³è¯†åˆ«å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨è¾“å…¥: ").strip()

    if not command_text:
        print("âš ï¸ æŒ‡ä»¤ä¸ºç©ºã€‚")
        return None
    print(f"âœ… æœ€ç»ˆæŒ‡ä»¤ï¼š{command_text}")

    # 2. é€šè¿‡å¤šæ¨¡æ€æ¨¡å‹è·å–æ£€æµ‹æ¡†
    result = generate_robot_actions(command_text, image_input)
    natural_response = result["response"]
    detection_info = result["coordinates"]
    print("è‡ªç„¶è¯­è¨€å›åº”ï¼š", natural_response)
    print("æ£€æµ‹åˆ°çš„ç‰©ä½“ä¿¡æ¯ï¼š", detection_info)

    # --- å…³é”®ä¿®æ”¹ï¼šè°ƒç”¨æ–°çš„ Edge-TTS æ’­æ”¾å‡½æ•° ---
    play_tts_edge(natural_response)
    # ----------------------------------------
    
    bbox = detection_info.get("bbox") if detection_info and "bbox" in detection_info else None
    
    # 3. å‡†å¤‡å›¾åƒä¾› SAM ä½¿ç”¨ï¼ˆè½¬æ¢ä¸º RGBï¼‰
    image_rgb = cv2.cvtColor(image_input, cv2.COLOR_BGR2RGB)

    # 4. åˆå§‹åŒ– SAMï¼Œå¹¶è®¾ç½®å›¾åƒ
    predictor = choose_model()
    predictor.set_image(image_rgb)

    if bbox:
        results = predictor(bboxes=[bbox])
        center, mask = process_sam_results(results)
        print(f"âœ… è‡ªåŠ¨æ£€æµ‹åˆ°ç›®æ ‡,bbox:{bbox}")
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ°ç›®æ ‡ï¼Œè¯·ç‚¹å‡»å›¾åƒé€‰æ‹©å¯¹è±¡")
        cv2.namedWindow('Select Object', cv2.WINDOW_NORMAL)
        cv2.imshow('Select Object', image_input)
        point = []

        def click_handler(event, x, y, flags, param):
            if event == cv2.EVENT_LBUTTONDOWN:
                point.extend([x, y])
                print(f"ğŸ–±ï¸ ç‚¹å‡»åæ ‡ï¼š{x}, {y}")
                cv2.setMouseCallback('Select Object', lambda *args: None)

        cv2.setMouseCallback('Select Object', click_handler)
        while True:
            key = cv2.waitKey(100)
            if point:
                break
            if cv2.getWindowProperty('Select Object', cv2.WND_PROP_VISIBLE) < 1:
                print("âŒ çª—å£è¢«å…³é—­ï¼Œæœªè¿›è¡Œç‚¹å‡»")
                return None
        cv2.destroyAllWindows()
        results = predictor(points=[point], labels=[1])
        center, mask = process_sam_results(results)

    # 5. ä¿å­˜åˆ†å‰²æ©ç 
    if mask is not None:
        cv2.imwrite(output_mask, mask, [cv2.IMWRITE_PNG_BILEVEL, 1])
        print(f"âœ… åˆ†å‰²æ©ç å·²ä¿å­˜ï¼š{output_mask}")
    else:
        print("âš ï¸ åˆ†å‰²å¤±è´¥ï¼Œæœªç”Ÿæˆæ©ç ")

    return mask


# ----------------------- ä¸»ç¨‹åºå…¥å£ -----------------------
if __name__ == '__main__':
    # è¯·ç¡®ä¿ç›®å½•ä¸‹æœ‰ sam_b.pt å’Œä¸€å¼ æµ‹è¯•å›¾ç‰‡
    # å¦‚æœæ²¡æœ‰å›¾ç‰‡ï¼Œè¯·æ›¿æ¢ä¸ºçœŸå®è·¯å¾„
    img_path = 'color_img_path.jpg' 
    
    # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å­˜åœ¨ï¼Œé¿å…ç›´æ¥æŠ¥é”™
    if os.path.exists(img_path):
        seg_mask = segment_image(img_path)
        print("Segmentation result mask shape:", seg_mask.shape if seg_mask is not None else None)
    else:
        print(f"âŒ æ‰¾ä¸åˆ°å›¾ç‰‡: {img_path}ï¼Œè¯·ä¿®æ”¹ä»£ç ä¸­çš„å›¾ç‰‡è·¯å¾„")