import cv2
import numpy as np
import torch
from ultralytics.models.sam import Predictor as SAMPredictor
from ultralytics import YOLOWorld
import os
import sys
import logging
import gc 
import time
import json
import re
import queue
import asyncio
import textwrap

# --- æ–°å¢ä¾èµ– (è¯·ç¡®ä¿å®‰è£…: pip install sounddevice soundfile scipy pydub edge-tts openai whisper) ---
import soundfile as sf
import sounddevice as sd
from scipy.io.wavfile import write
from pydub import AudioSegment
import edge_tts
import whisper
from openai import OpenAI
from dotenv import load_dotenv

# ç¦ç”¨ Ultralytics å†—ä½™æ—¥å¿—
logging.getLogger("ultralytics").setLevel(logging.WARNING)

# ================= 0. å…¨å±€é…ç½®åŒº (åœ¨æ­¤å¤„æ‰‹åŠ¨ä¿®æ”¹) =================

# [è¾“å…¥æ¨¡å¼é€‰æ‹©] True = ä½¿ç”¨éº¦å…‹é£è¯­éŸ³è¾“å…¥; False = ä½¿ç”¨é”®ç›˜æ–‡å­—è¾“å…¥
USE_VOICE_INPUT = False

# [è¯­éŸ³å›å¤å¼€å…³] True = å¯ç”¨ Edge-TTS è¯­éŸ³æ’­æŠ¥; False = ä»…åœ¨ç»ˆç«¯æ‰“å°æ–‡å­—
ENABLE_TTS_REPLY = False

# ===================================================================

# åŠ è½½ .env ç¯å¢ƒå˜é‡
current_dir = os.path.dirname(os.path.abspath(__file__))
env_path = os.path.join(current_dir, '.env')
if os.path.exists(env_path):
    load_dotenv(env_path)
    print(f"âœ… å·²åŠ è½½é…ç½®æ–‡ä»¶: {env_path}")
else:
    print(f"âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶: {env_path}")

# å…¨å±€æ¨¡å‹ç¼“å­˜
_yolo_model = None
_sam_predictor = None
_whisper_model = None

# éŸ³é¢‘å‚æ•°
samplerate = 48000
channels = 1
dtype = 'int16'
q = queue.Queue()

# ================= 1. è¯­éŸ³ä¸ TTS æ¨¡å— =================

def load_whisper():
    global _whisper_model
    if _whisper_model is None:
        print("ğŸ”„ æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹...")
        # å¯æ ¹æ®æ˜¾å­˜æ”¹ä¸º "base" æˆ– "tiny" åŠ å¿«é€Ÿåº¦
        _whisper_model = whisper.load_model("small") 
        print("âœ… Whisper åŠ è½½å®Œæ¯•")
    return _whisper_model

def callback(indata, frames, time_info, status):
    if status:
        print("âš ï¸ çŠ¶æ€è­¦å‘Šï¼š", status)
    q.put(bytes(indata))

def rms(audio_frame):
    samples = np.frombuffer(audio_frame, dtype=np.int16)
    if samples.size == 0: return 0
    mean_square = np.mean(samples.astype(np.float32) ** 2)
    return np.sqrt(mean_square)

def recognize_speech():
    """ æ™ºèƒ½è¯­éŸ³ç›‘å¬ (è‡ªé€‚åº”åº•å™ª) """
    DEVICE_ID = 13  # <--- è¯·æ ¹æ®å®é™…è®¾å¤‡ ID ä¿®æ”¹ (python -m sounddevice)
    
    # é˜ˆå€¼å‚æ•°
    NOISE_MARGIN = 500
    MIN_SAFE_THRESHOLD = 7600
    MAX_SAFE_THRESHOLD = 8400
    BUFFER_DURATION = 1.0
    CALIBRATION_TIME = 2.0
    SILENCE_TIMEOUT = 1.2
    
    local_frame_samples = int(BUFFER_DURATION * samplerate)
    with q.mutex: q.queue.clear()

    print("\n" + "="*40)
    print("ğŸ”‡ æ­£åœ¨æµ‹é‡ç¯å¢ƒåº•å™ª (è¯·ä¿æŒå®‰é™)...")
    
    noise_values = []
    try:
        # --- é˜¶æ®µ 1: åº•å™ªæ ¡å‡† ---
        with sd.RawInputStream(samplerate=samplerate, blocksize=local_frame_samples,
                               device=DEVICE_ID, latency='high',
                               dtype=dtype, channels=channels, callback=callback):
            time.sleep(0.5)
            for _ in range(int(CALIBRATION_TIME / BUFFER_DURATION)):
                if not q.empty():
                    val = rms(q.get())
                    noise_values.append(val)
                else:
                    time.sleep(BUFFER_DURATION)
        
        avg_noise = np.mean(noise_values) if noise_values else 7500
        final_threshold = max(avg_noise + NOISE_MARGIN, MIN_SAFE_THRESHOLD)
        final_threshold = min(final_threshold, MAX_SAFE_THRESHOLD)
        
        print(f"âœ… åº•å™ª: {int(avg_noise)} | ğŸ¯ è§¦å‘é˜ˆå€¼: {int(final_threshold)}")
        print("ğŸ¤ è¯·è¯´è¯...")

        # --- é˜¶æ®µ 2: ç›‘å¬å½•éŸ³ ---
        audio_buffer = []
        is_speaking = False
        last_voice_time = time.time()
        
        with sd.RawInputStream(samplerate=samplerate, blocksize=local_frame_samples,
                               device=DEVICE_ID, latency='high',
                               dtype=dtype, channels=channels, callback=callback):
            while True:
                frame = q.get()
                volume = rms(frame)
                current_time = time.time()
                
                # å¯è§†åŒ–è¿›åº¦æ¡
                display_vol = max(0, volume - 7000)
                bar_len = min(int((display_vol / 2000) * 20), 20)
                status_icon = "ğŸ”´ REC" if is_speaking else "ğŸ‘‚ WAIT"
                print(f"\r   {status_icon} |{'â–ˆ' * bar_len:<20}| {int(volume)}", end="")

                if volume > final_threshold:
                    is_speaking = True
                    audio_buffer.append(np.frombuffer(frame, dtype=np.int16))
                    last_voice_time = current_time
                else:
                    if is_speaking:
                        audio_buffer.append(np.frombuffer(frame, dtype=np.int16))
                        if current_time - last_voice_time > SILENCE_TIMEOUT:
                            print("\nâœ… å½•éŸ³ç»“æŸ")
                            return np.concatenate(audio_buffer, axis=0)
                    elif (current_time - last_voice_time > 30.0):
                        return np.array([], dtype=np.int16) # è¶…æ—¶

    except Exception as e:
        print(f"\nâŒ éº¦å…‹é£é”™è¯¯: {e}")
        return np.array([], dtype=np.int16)

def speech_to_text(audio_data):
    if len(audio_data) == 0: return ""
    model = load_whisper()
    temp_wav = "temp_audio.wav"
    write(temp_wav, samplerate, audio_data.astype(np.int16))
    try:
        # ä½¿ç”¨ fp16=False å…¼å®¹æ€§æ›´å¥½
        result = model.transcribe(temp_wav, language="zh", fp16=torch.cuda.is_available())
        text = result["text"].strip()
        print(f"\nğŸ“ è¯­éŸ³è¯†åˆ«ç»“æœ: {text}")
        return text
    except Exception as e:
        print(f"âŒ è¯†åˆ«å¤±è´¥: {e}")
        return ""

async def _edge_tts_generate(text, output_file, voice="zh-CN-XiaoxiaoNeural"):
    communicate = edge_tts.Communicate(text, voice)
    await communicate.save(output_file)

def play_tts(text):
    if not ENABLE_TTS_REPLY:
        print(f"ğŸ”‡ (TTSå·²ç¦ç”¨) ç³»ç»Ÿå›å¤: {text}")
        return
        
    if not text: return
    print(f"ğŸ“¢ æ­£åœ¨æ’­æŠ¥: {text}")
    temp_mp3 = "temp_tts.mp3"
    try:
        asyncio.run(_edge_tts_generate(text, temp_mp3))
        audio = AudioSegment.from_mp3(temp_mp3)
        # å¼ºåˆ¶é‡é‡‡æ ·åˆ° 48000Hz é¿å…å£°å¡æŠ¥é”™
        audio = audio.set_frame_rate(48000).set_channels(1)
        data = np.array(audio.get_array_of_samples())
        sd.play(data, 48000)
        sd.wait()
    except Exception as e:
        print(f"âŒ TTS å¤±è´¥: {e}")
    finally:
        if os.path.exists(temp_mp3): os.remove(temp_mp3)

# ================= 2. å¤§æ¨¡å‹è¯­ä¹‰æå– (çº¯æ–‡æœ¬) =================

def extract_object_name_with_llm(user_text):
    """
    è°ƒç”¨ Qwen (çº¯æ–‡æœ¬æ¨¡å¼) å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤è½¬æ¢ä¸º YOLO-World å¯ç”¨çš„ç‰©ä½“åç§°ã€‚
    æ— è®ºè¾“å…¥æ˜¯è¯­éŸ³è½¬çš„æ–‡å­—ï¼Œè¿˜æ˜¯æ‰‹åŠ¨æ‰“çš„æ–‡å­—ï¼Œéƒ½ç»è¿‡è¿™é‡Œã€‚
    """
    api_key = os.getenv("QWEN_API_KEY")
    if not api_key:
        print("âš ï¸ æœªé…ç½® QWEN_API_KEYï¼Œè·³è¿‡ LLM è§£æï¼Œç›´æ¥ä½¿ç”¨è¾“å…¥æ–‡æœ¬ã€‚")
        return user_text, "å¥½çš„ã€‚"

    # ä½¿ç”¨å…¼å®¹ OpenAI åè®®çš„å®¢æˆ·ç«¯
    client = OpenAI(api_key=api_key, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")

    # ä¸“é—¨ä¸º YOLO-World ä¼˜åŒ–çš„ Prompt
    system_prompt = textwrap.dedent("""\
    ä½ æ˜¯ä¸€ä¸ªæŠ“å–é¡¹ç›®çš„ç‰©ä½“æ£€æµ‹åŠ©æ‰‹ã€‚ä»»åŠ¡æ˜¯ä»ç”¨æˆ·çš„ä¸­æ–‡æŒ‡ä»¤ä¸­æå–ç”¨äº YOLO-World æ£€æµ‹çš„ã€è‹±æ–‡ç‰©ä½“åç§°ã€‘ã€‚

    ã€è§„åˆ™ã€‘
    1. **åªè¾“å‡ºJSON**ï¼Œæ— å…¶ä»–å†…å®¹ã€‚
    2. JSONåŒ…å«:
       - "object_en": ç‰©ä½“è‹±æ–‡å (å¦‚ "red apple", "blue bottle")ã€‚
       - "reply_cn": ç®€çŸ­ä¸­æ–‡å›å¤ (å¦‚ "å¥½çš„ï¼Œæ­£åœ¨æ‰¾çº¢è‹¹æœ")ã€‚
    3. ä½ å…·å¤‡è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡åˆ¤æ–­ç”¨æˆ·æƒ³è¦çš„ç‰©ä½“ï¼Œä¾‹å¦‚ï¼šå¯Œå«è†³é£Ÿçº¤ç»´çš„æ°´æœ -> â€œyellow bananaâ€ã€‚
    4. å›¾ä¸­ä»…åŒ…å«è¿™äº›ç‰©ä½“: red apple,yellow banana,yellow duck toy,mouse,combination of hammer head and handle,ä½ åªéœ€è¦ä»ä¸­é€‰æ‹©æœ€ç›¸å…³çš„ç‰©ä½“åç§°ã€‚
    5. è¿™æ˜¯ä¸€ä¸ªæŠ“å–ä»»åŠ¡ï¼Œä¸Šè¿°ç‰©ä½“éƒ½æ˜¯æ¡Œé¢ä¸Šçš„å¸¸è§ç‰©å“ï¼Œæ³¨æ„åˆ†è¾¨è‹±æ–‡å•è¯çš„å«ä¹‰ï¼Œä¾‹å¦‚mouseæ˜¯â€œé¼ æ ‡â€ä¸æ˜¯â€œè€é¼ â€ã€‚                              
    6. reply_cnçš„å›å¤ç®€çŸ­ï¼å¯ä»¥ä¿çš®ä¸€ç‚¹ã€ä½†å¿…é¡»åªç¡®è®¤è¢«é€‰ä¸­çš„ç›®æ ‡ï¼Œä¸è¦å•°å—¦ã€‚
    7. å¦‚æœç”¨æˆ·æŒ‡ä»¤ä¸ä¸Šè¿°ç‰©ä½“å®Œå…¨æ— å…³ï¼Œ"reply_cn" è¯´æ˜ä¸ç†è§£ç”¨æˆ·æ„å›¾æˆ–è€…æ— æ³•æ‰¾åˆ°å¯¹åº”ç‰©ä½“ï¼Œ"object_en" ä¸ºç©ºå­—ç¬¦ä¸²ã€‚  

    ã€ç¤ºä¾‹ã€‘
    ç”¨æˆ·: "æˆ‘è¦åƒçº¢è‰²æ°´æœ"
    è¾“å‡º: {"object_en": "red apple", "reply_cn": "æ”¶åˆ°ï¼å·²ç»é”å®šé‚£ä¸ªè‹¹æœå•¦ï¼Œå‡†å¤‡æŠ“å–ã€‚"}
    """)

    try:
        completion = client.chat.completions.create(
            model="qwen-plus", # çº¯æ–‡æœ¬æ¨¡å‹ï¼Œé€Ÿåº¦å¿«
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_text}
            ],
            temperature=0.1
        )
        content = completion.choices[0].message.content
        
        # è§£æ JSON
        match = re.search(r'(\{.*\})', content, re.DOTALL)
        if match:
            data = json.loads(match.group(1))
            return data.get("object_en", user_text), data.get("reply_cn", "æ”¶åˆ°æŒ‡ä»¤ã€‚")
        else:
            return user_text, "æ”¶åˆ°ã€‚"
            
    except Exception as e:
        print(f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
        # é™çº§å¤„ç†ï¼šç›´æ¥è¿”å›åŸæ–‡
        return user_text, "ç½‘ç»œä¼¼ä¹æœ‰ç‚¹é—®é¢˜ï¼Œæˆ‘ç›´æ¥è¯•è¯•ã€‚"

# ================= 3. YOLO & SAM æ ¸å¿ƒé€»è¾‘ =================

def get_yolo_model():
    global _yolo_model
    if _yolo_model is None:
        _yolo_model = YOLOWorld('yolov8l-worldv2.pt')
    return _yolo_model

def get_sam_predictor():
    global _sam_predictor
    if _sam_predictor is None:
        model_weight = 'sam_b.pt'
        if not os.path.exists(model_weight):
            alt_path = os.path.join(os.path.dirname(__file__), '../../sam_b.pt')
            if os.path.exists(alt_path): model_weight = alt_path
        overrides = dict(task='segment', mode='predict', model=model_weight, conf=0.25, save=False)
        _sam_predictor = SAMPredictor(overrides=overrides)
    return _sam_predictor

def process_sam_results(results):
    if not results or not results[0].masks: return None, None
    mask = results[0].masks.data[0].cpu().numpy()
    mask = (mask > 0).astype(np.uint8) * 255
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours: return None, None
    M = cv2.moments(contours[0])
    if M["m00"] == 0: return None, mask
    cx = int(M["m10"] / M["m00"])
    cy = int(M["m01"] / M["m00"])
    return (cx, cy), mask

# ================= 4. ä¸»æµç¨‹ =================

def segment_image(image_input, output_mask='mask1.png'):
    """
    æ•´åˆä¸»å‡½æ•°ï¼š
    1. æ ¹æ® USE_VOICE_INPUT é€‰æ‹©è¾“å…¥æ–¹å¼
    2. å°†è¾“å…¥æ–‡æœ¬é€å…¥ LLM æå–ç‰©ä½“å
    3. TTS æ’­æŠ¥å›å¤
    4. YOLO æ£€æµ‹ -> SAM åˆ†å‰²
    
    ã€å…³é”®ä¿®æ”¹ã€‘è¿”å›å€¼æ”¹ä¸ºå…ƒç»„: (mask, target_obj_name)
    """
    # 0. æ¸…ç†æ˜¾å­˜
    gc.collect()
    torch.cuda.empty_cache()

    if isinstance(image_input, str):
        image_input = cv2.imread(image_input)
        if image_input is None: return None, "" # <--- ä¿®æ”¹1: è¿”å›ç©ºå…ƒç»„

    # 1. è·å–ç”¨æˆ·æŒ‡ä»¤
    print("\n" + "="*40)
    user_text = ""
    
    if USE_VOICE_INPUT:
        print("ğŸ¤– [è¯­éŸ³æ¨¡å¼] è¯·è¯´è¯ (Ctrl+C å¯ä¸­æ–­)...")
        try:
            audio = recognize_speech()
            if len(audio) > 0:
                user_text = speech_to_text(audio)
            else:
                print("âš ï¸ æœªæ£€æµ‹åˆ°æœ‰æ•ˆå£°éŸ³ã€‚")
        except KeyboardInterrupt:
            print("\nâš ï¸ è¯­éŸ³è¢«ä¸­æ–­ï¼Œåˆ‡æ¢ä¸ºæ‰‹åŠ¨è¾“å…¥ã€‚")
            user_text = input("ğŸ‘‰ è¯·è¾“å…¥æŒ‡ä»¤: ").strip()
    else:
        print("âŒ¨ï¸  [æ–‡å­—æ¨¡å¼]")
        user_text = input("ğŸ‘‰ è¯·è¾“å…¥æŒ‡ä»¤ (ä¾‹å¦‚ 'æŠ“å–çº¢è‰²çš„è‹¹æœ'): ").strip()

    if not user_text:
        print("âŒ æŒ‡ä»¤ä¸ºç©ºï¼Œæ“ä½œå–æ¶ˆã€‚")
        return None, "" # <--- ä¿®æ”¹2

    # 2. LLM è¯­ä¹‰è§£æ (æ— è®ºè¯­éŸ³è¿˜æ˜¯æ–‡å­—ï¼Œéƒ½ç»è¿‡è¿™é‡Œ)
    print(f"ğŸ¤” æ­£åœ¨è§£ææŒ‡ä»¤: \"{user_text}\" ...")
    target_obj_name, reply_text = extract_object_name_with_llm(user_text)
    
    print(f"ğŸ¯ æå–ç›®æ ‡: [{target_obj_name}]")
    print(f"ğŸ¤– ç³»ç»Ÿå›å¤: \"{reply_text}\"")
    
    # 3. è¯­éŸ³å›å¤
    play_tts(reply_text)

    # 4. YOLO æ£€æµ‹
    print(f"ğŸ” YOLO-World æ­£åœ¨æœç´¢: '{target_obj_name}' ...")
    model = get_yolo_model()
    model.set_classes([target_obj_name])
    
    with torch.no_grad():
        results = model.predict(image_input, conf=0.05, iou=0.5, verbose=False)
    
    bbox = None
    if len(results) > 0 and len(results[0].boxes) > 0:
        best_box = results[0].boxes[0]
        coords = best_box.xyxy[0].cpu().numpy().astype(int)
        conf = float(best_box.conf)
        bbox = coords.tolist()
        print(f"âœ… æ‰¾åˆ°ç›®æ ‡! ç½®ä¿¡åº¦: {conf:.2f}")
    else:
        print(f"âŒ æœªæ‰¾åˆ°ç›®æ ‡: '{target_obj_name}'")
        play_tts(f"æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°{target_obj_name}ã€‚")
        return None, "" # <--- ä¿®æ”¹3

    # 5. å¯è§†åŒ– YOLO ç»“æœ
    if bbox:
        try:
            vis_img = image_input.copy()
            cv2.rectangle(vis_img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)
            # é˜²æ­¢æ–‡å­—å‡ºç•Œ
            text_y = bbox[1] - 10 if bbox[1] - 10 > 10 else bbox[1] + 20
            cv2.putText(vis_img, f"{target_obj_name}", (bbox[0], text_y), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
            cv2.imwrite("debug_detection.jpg", vis_img)
        except: pass

    # 6. SAM åˆ†å‰²
    print("ğŸ”„ å¯åŠ¨ SAM åˆ†å‰²...")
    try:
        image_rgb = cv2.cvtColor(image_input, cv2.COLOR_BGR2RGB)
        predictor = get_sam_predictor()
        
        with torch.no_grad():
            predictor.set_image(image_rgb)
            center_x = (bbox[0] + bbox[2]) / 2
            center_y = (bbox[1] + bbox[3]) / 2
            results = predictor(bboxes=[bbox], points=[[center_x, center_y]], labels=[1])
            
        _, mask = process_sam_results(results)
        del results
        
    except Exception as e:
        print(f"âš ï¸ SAM è¿è¡Œå‡ºé”™: {e}")
        return None, "" # <--- ä¿®æ”¹4

    if mask is not None:
        cv2.imwrite(output_mask, mask, [cv2.IMWRITE_PNG_BILEVEL, 1])
        print(f"âœ… æ©ç å·²ä¿å­˜")
    
    gc.collect()
    torch.cuda.empty_cache()
    
    # <--- ä¿®æ”¹5: è¿”å› mask å’Œ è‹±æ–‡ç‰©ä½“å
    return mask, target_obj_name

# å…¼å®¹æ¥å£
def choose_model(): return get_sam_predictor()
def generate_robot_actions(*args): return {}
def play_tts_edge(*args): pass